<!DOCTYPE html>
<html manifest="/fakeApp/app.cache">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Generated with http://realfavicongenerator.net/
  So good, so easy! -->
  <link rel="apple-touch-icon" sizes="57x57"   href="/static/app-icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60"   href="/static/app-icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72"   href="/static/app-icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76"   href="/static/app-icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/static/app-icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/static/app-icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/static/app-icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/static/app-icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/app-icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png"            href="/static/app-icons/favicon-32x32.png"          sizes="32x32">
  <link rel="icon" type="image/png"            href="/static/app-icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png"            href="/static/app-icons/favicon-96x96.png"          sizes="96x96">
  <link rel="icon" type="image/png"            href="/static/app-icons/favicon-16x16.png"          sizes="16x16">
  <link rel="manifest" href="/static/app-icons/manifest.json">
  <link rel="shortcut icon" href="/static/app-icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#b91d47">
  <meta name="msapplication-TileImage" content="/static/app-icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/static/app-icons/browserconfig.xml">
  <meta name="theme-color" content="#999999">



  <title>Craftspeople of the Future</title>
  <meta name="description" content="Futures Forum
">

    <style>/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type=checkbox],input[type=radio]{box-sizing:border-box;padding:0}input[type=number]::-webkit-inner-spin-button,input[type=number]::-webkit-outer-spin-button{height:auto}input[type=search]{-webkit-appearance:textfield;box-sizing:content-box}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style>
           <link rel="stylesheet" href="/assets/main-0f9a4b7616c1f0884713f0891c27ff06.css">
  

  <link rel="canonical" href="http://bvn.camp//essays/craftspeople-of-the-future.html">
 
</head>


  <body>
    <div class="menu-button">
      <a href="#">menu</a>
</div>

<nav class="in">
    <div class="menu-items">
        <ul>
            <li><a href="/">Programme</a></li>
            <li><a href="/icons/index.html">Icons</a></li>
            <li><a href="/essays/">Essays</a></li>
            <li><a href="/other-pages/bigquestions.html">Big Questions</a></li>
            <!-- <li><a href="/other-pages/map.html">Map</a></li> -->
            <li><a href="/other-pages/speaker-bios.html">Speaker Bios</a></li>
            <li><a href="/other-pages/simple-programme.html">Quick programme</a></li>
        </ul>
    </div>
</nav>
    <article class="essay">
    <h2 id="the-problem-of-interface">THE PROBLEM OF INTERFACE</h2>

<p>Our hands are a wonderful tool for crafting and forming physical objects.</p>

<p>Unfortunately we cannot access their full potential when we design on a computer.</p>

<p>This essay seeks to question the way our body interacts with the design process. It imagines a future where the promise of the digital is no longer restrained by the mouse and keyboard. A future where our hands’ fine motor control and sense of touch can be used to mould, shape and craft a digitally designed artefact.</p>

<p>We rely on our brain to abstract and visualise good design when using a computer. We limit the full potential of our hands by using the mouse and keyboard. We use a terrible interface to control the most powerful man-made tool the brain has ever had access to. Wasting the capacity to use the embodied knowledge of our hands and body to provide highly complex mastery over a tool and medium.</p>

<p>We should be liberating ourselves in how we control the computer.</p>

<h2 id="crafting-the-workbench-of-the-future">CRAFTING THE WORKBENCH OF THE FUTURE</h2>

<p>How can we release digital models from the flatness of a screen and into three dimensional physical reality?</p>

<p>How can we use the muscle memory of our hands to form a digital representation of spatial and textural composition?</p>

<p>Could a device reduce abstraction in the design process and allow us to go directly to sculpting three dimensional architecture?</p>

<p>Rapid progress is being made in the development of equipment that allows the computer to track the human body. Our smart phones are equipped with sensitive motion sensors. It has become second nature to rotate our screens and to use touch control combinations. The Kinect, TrackIR and Leap Motion give us the ability to rapidly and precisely track the movements of our hands and bodies in a three dimensional space.</p>

<p>The tools that could allow us to sculpt digital geometry with our hands already exist. Why have these not yet superseded the mouse and keyboard as control interfaces? Maybe it is because we are creating a flattened representation of three dimensional geometry on a computer screen.</p>

<p>The missing ingredient is three dimensional representation of the digital artefact in space. There are technologies being developed to address this limitation. These include augmented reality, virtual reality and holographic projection. Holographic projection is an exciting option, it would allow a digitally designed model to explode out of the computer screen and occupy physical space. It would allow people to experience and view the design in a collaborative environment. Multiple designers could work on the same spatial representation of a product at the same time.</p>

<h2 id="an-element-of-haptic-feedback">AN ELEMENT OF HAPTIC FEEDBACK</h2>

<p>The big problem with a holographic projection is that it doesn’t feel physical. The sense of touch is lost. There is no opportunity for haptic feedback, users are unable to train their muscle memory and learn from the formation of the artefact.</p>

<p>A mechanical exoskeleton for the hands could enable this sensory feedback. Combined with sensors to detect our hands and fingers in space, the exoskeleton could exert varying degrees of resistance or vibration to create the illusion of physical presence. This would allow us to push, squeeze, prod, pinch, drag and rotate the holographic representation and sense it through touch as if it were real.</p>

<p>Although lacking the charm of old school craftsmanship, the combination of holographic projection coupled with a haptic feedback mechanism would enable us to craft a digital object with the realistic and natural movement of our hands.</p>

<h2 id="workshop-based-architectural-practice">WORKSHOP BASED ARCHITECTURAL PRACTICE</h2>

<p>This computational crafting workbench would allow us to throw out most of the desks and screens we use day to day.</p>

<p>In place of these, we could stand up and move around our workbench, using tools to drill, texture, polish and cut our digital artefacts. A structural armature would frame each workbench to support the exoskeleton, spatial sensors and holographic projectors. The space would more closely resemble a high-tech manufacturing facility or workshop than an open plan office.</p>

<p>Alternatively, the projection could come from a clothing item or headset that would allow us to walk around and develop our design anywhere we like. It would allow us to design on the bus, to sit in a quiet, dimly lit space to concentrate or to project the design into the centre of a communal space for public feedback.</p>

<p>Or perhaps we could all lie back on comfortable seats and wear something that looks like this:</p>

<picture>
    <source srcset="/generated/essays/image003-327by297-f41f92.png" media="(min-width: 40em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 40em) and (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/image003-327by297-f41f92.png" media="(min-width: 40em)" />
    <source srcset="/generated/essays/image003-327by297-f41f92.png" media="(min-width: 30em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 30em) and (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/image003-327by297-f41f92.png" media="(min-width: 30em)" />
    <source srcset="/generated/essays/image003-327by187-f41f92.png" media="(-webkit-min-device-pixel-ratio: 1.5), (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/image003-327by187-f41f92.png" />
    <img srcset="/generated/essays/image003-327by187-f41f92.png" class="blog-full" itemprop="image" alt="" title="" />
  </picture>

<ol>
	<li><span class="commenter">Ben Doherty</span>
	I really like the idea of working with virtual materials, but while still using our whole bodies. There is a lot of work on embodied interfaces and it’s really interesting (<a href="http://www.autodeskresearch.com/publications" rel="nofollow">Autodesk research tape drawing</a>, Andrew Burrow’s tangible wiki, at least one episode of startrek). 

	There is also the idea that we can have ‘materials’ that don’t follow our regular physics. Daniel Piker talks about <a href="https://spacesymmetrystructure.wordpress.com/2011/05/18/pseudo-physical-materials/" rel="nofollow">pseudo-physical materials</a> and how they can augment our thinking. If we could mix tangible interfaces and magic substances we might be onto something!

	The idea of the ‘rig’ is pretty nicely explored in <a href="http://www.amazon.com/Ready-Player-One-Ernest-Cline-ebook/dp/B005CVWWJY/ref=sr_1_1?ie=UTF8&amp;qid=1424050389&amp;sr=8-1&amp;keywords=ready+player+one" rel="nofollow">Ready Player One</a>. There are lots of new technologies (shape memory alloys etc.) that might mature in the next 15 years. It’s not far off being reality.

	Watching a group of people around a large model physical is really interesting. If that model could be manipulated by all members, and the context/perspective changed it might make the communication and coordination part of designing things thousands of times faster. “Just <em>show</em> me…”

	The thing that I think might be interesting is how we define new materials. How do we decide where to draw the line between abstraction and reality?

</li>
<li><span class="commenter">David McGirr</span>
	Its a provocative notion: Using technology to ameliorate a situation technology itself produced, analogous to  using climate engineering to overcome climate change (“science created the problem, but science will fix it”).<br />
The counter argument is of course that application of this technology will exacerbate the problem; drive the wedge further between what we make and how we make it (that’s my immediate instinctive reaction looking at the image provided, although instincts aren’t always to be trusted).<br />
Also I’d like to make a devils advocate plug for abstraction (abstraction being the obstacle represented by a flat computer screen overcome by VR/AR whatever): Abstraction isn’t always a bad thing, abstraction allows for particular focus on things (through reduction, simplification) in the creative process. For example where would be without the Cartesian abstraction of a building to floor plan and section (yeah yeah it probably created a lot of problems and bad design too but its  a major development in understanding form visually).

</li>
<li><span class="commenter">Ben Doherty</span>
	<a href="https://www.kickstarter.com/projects/therealmsystem/the-realm-system?ref=category_recommended" rel="nofollow">The REALM System</a>

	<picture>
    <source srcset="/generated/essays/realm-700by522-53b364.jpg" media="(min-width: 40em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 40em) and (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/realm-700by522-53b364.jpg" media="(min-width: 40em)" />
    <source srcset="/generated/essays/realm-675by503-53b364.jpg" media="(min-width: 30em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 30em) and (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/realm-450by336-53b364.jpg" media="(min-width: 30em)" />
    <source srcset="/generated/essays/realm-525by300-53b364.jpg" media="(-webkit-min-device-pixel-ratio: 1.5), (min-resolution: 144dpi)" />
    <source srcset="/generated/essays/realm-350by200-53b364.jpg" />
    <img srcset="/generated/essays/realm-350by200-53b364.jpg" class="blog-full" itemprop="image" alt="" title="" />
  </picture>


	It’s coming!

</li>
<li><span class="commenter">Ben Doherty</span>
	I think the point of this is that the abstraction is also being designed. You aren’t forced to use a fixed set of abstractions. The user can change point of view or interaction mode at will.

	The main advantage that I see from something like this is that it enables the user’s fine motor skills. <a href="http://dougengelbart.org/" rel="nofollow">People have known from the start</a> that the mouse and keyboard are a sub-par interface. I think that your analogy of climate engineering is misleading. The interface problem isn’t one that needs to be ameliorated as it leaves no lasting change. Once we change interface there is an immediate improvement. (Excepting a small relearning phase.)

</li>
</ol>

    </article>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $('.menu-button').on('click tap', function(e) {
        e.preventDefault();
        $("nav").toggleClass("in");
    });

    var selectedCardManager = new function() {
      var selectedCard = null;
      this.removeCurrentCard = function() {
        $(selectedCard).children('.ff-twitter-widget').children().remove();
        selectedCard = null;
        if (typeof setCardState === "function") { setCardState("out"); }
        $(".front").scrollTop(0);
      };
      this.setCurrentCard = function(newCard) {
        selectedCard = newCard;
        var twitterWidgetParent = $(selectedCard).find('.ff-twitter-widget');
        var twitterWidget = $('<a href="">')
          .addClass('twitter-timeline')
          .attr('width','')
          .attr('height','')
          .attr('data-widget-id', twitterWidgetParent.data('widget-id'))
          .attr('data-theme','light')
          .attr('data-link-color','rgba(150,150,150,1)')
          .attr('data-chrome','noheader nofooter noborders transparent noscrollbar')
          .attr('data-tweet-limit','20')
          .attr('data-aria-polite','polite')[0];
        // console.log('created: ', twitterWidget);
        twitterWidgetParent.append(twitterWidget);
        twttr.widgets.load(twitterWidgetParent[0]);
        // console.log('appended to: ', twitterWidgetParent);
        if (typeof setCardState === "function") { setCardState("in"); }
      };
      this.manage = function(newCard) {
        console.log('Managing:', newCard);
        if (selectedCard === null) {
          console.log('setting current card');
          this.setCurrentCard(newCard);
        } else {
          console.log('removing current card');
          this.removeCurrentCard();
        }
      };
    }();

    $('.card').on('click tap', function(e) {
        selectedCardManager.manage(this);

        $(this).toggleClass("live-one");
        $('html, body').animate({
            scrollTop: $(this).offset().top
        }, 300);
        $("body").toggleClass("noscroll");
    });
    
    $(".menu-items a").each(function() {
        if ($(this).attr("href") === window.location.pathname) {
            $(this).parent().addClass("on-this-page");
        }
    });

});
</script>

<script src="http://platform.twitter.com/widgets.js"></script>


  </body>

</html>
